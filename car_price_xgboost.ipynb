{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Car Price Prediction with XGBoost\n",
        "\n",
        "This notebook trains and evaluates an **XGBoost (XGBRegressor)** model to predict car prices using the `cleaned_autocentral_data.csv` dataset.\n",
        "\n",
        "We will:\n",
        "- Load and inspect the data\n",
        "- Separate **features (X)** and **target (y)**\n",
        "- Preprocess data with **One-Hot Encoding** for categorical features and **scaling** for numeric ones (via `ColumnTransformer` and `Pipeline`)\n",
        "- Split into **train/test** sets with a fixed random state\n",
        "- Train an **XGBoost regressor** with reasonable default hyperparameters\n",
        "- **Tune hyperparameters** using `RandomizedSearchCV`\n",
        "- Evaluate the model using **RMSE**, **MAE**, and **R²**, and briefly interpret the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If XGBoost is not installed, run this (uncomment the next line):\n",
        "# !pip install xgboost scikit-learn pandas numpy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "DATA_PATH = \"cleaned_autocentral_data.csv\"\n",
        "TARGET_COLUMN = \"price\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset and separate features/target\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "if TARGET_COLUMN not in df.columns:\n",
        "    raise ValueError(f\"Target column '{TARGET_COLUMN}' not found in dataset.\")\n",
        "\n",
        "X = df.drop(columns=[TARGET_COLUMN])\n",
        "y = df[TARGET_COLUMN]\n",
        "\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(\"Features shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "print(\"Columns:\", list(X.columns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build preprocessing: One-Hot Encode categoricals, scale numerics\n",
        "\n",
        "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "numeric_features = X.select_dtypes(exclude=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "print(\"Numeric features:\", numeric_features)\n",
        "\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "numeric_transformer = StandardScaler()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "print(\"Train size:\", X_train.shape[0])\n",
        "print(\"Test size:\", X_test.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build XGBoost pipeline with reasonable default hyperparameters\n",
        "\n",
        "xgb_reg = XGBRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    objective=\"reg:squarederror\",\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    eval_metric=\"rmse\",  # Evaluation metric during training\n",
        ")\n",
        "\n",
        "base_model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"regressor\", xgb_reg),\n",
        "    ]\n",
        ")\n",
        "\n",
        "base_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning with RandomizedSearchCV\n",
        "\n",
        "param_distributions = {\n",
        "    \"regressor__n_estimators\": [200, 300, 500, 800],\n",
        "    \"regressor__max_depth\": [3, 4, 6, 8],\n",
        "    \"regressor__learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
        "    \"regressor__subsample\": [0.6, 0.8, 1.0],\n",
        "    \"regressor__colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    scoring=\"neg_root_mean_squared_error\",\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "print(\"Starting hyperparameter tuning (RandomizedSearchCV)...\")\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best CV RMSE (negated): {-search.best_score_:,.2f}\")\n",
        "print(\"Best hyperparameters:\")\n",
        "for param, value in search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "best_model = search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate best XGBoost model on the test set\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"=== XGBoost Regression: Car Price Prediction ===\")\n",
        "print(f\"Number of samples (train): {len(X_train)}\")\n",
        "print(f\"Number of samples (test) : {len(X_test)}\")\n",
        "print()\n",
        "print(\"Performance on test set:\")\n",
        "print(f\"- RMSE (Root Mean Squared Error): {rmse:,.2f}\")\n",
        "print(f\"- MAE  (Mean Absolute Error)    : {mae:,.2f}\")\n",
        "print(f\"- R² Score                      : {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation of metrics\n",
        "\n",
        "- **RMSE (Root Mean Squared Error)**: Penalizes larger errors more strongly. A lower RMSE means large price mistakes are relatively rare.\n",
        "- **MAE (Mean Absolute Error)**: Average absolute difference between predicted and true prices, in the same units as the target. A lower MAE means predictions are closer on average.\n",
        "- **R² Score**: Proportion of variance in car prices explained by the model (between 0 and 1). Values closer to 1 indicate a better fit.\n",
        "\n",
        "You can compare these metrics with other models (e.g., CatBoost) to see which algorithm performs best on this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained best XGBoost model to disk\n",
        "\n",
        "import joblib\n",
        "\n",
        "MODEL_PATH_XGB = \"xgboost_car_price_model.pkl\"\n",
        "\n",
        "joblib.dump(best_model, MODEL_PATH_XGB)\n",
        "print(f\"Saved XGBoost model to {MODEL_PATH_XGB}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
